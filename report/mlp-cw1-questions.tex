%% REPLACE sXXXXXXX with your student number
\def\studentNumber{s1720422}


%% START of YOUR ANSWERS
%% Add answers to the questions below, by replacing the text inside the brackets {} for \youranswer{ "Text to be replaced with your answer." }. 
%
% Do not delete the commands for adding figures and tables. Instead fill in the missing values with your experiment results, and replace the images with your own respective figures.
%
% You can generally delete the placeholder text, such as for example the text "Question Figure 2 - Replace the images ..." 
%
% There are 19 TEXT QUESTIONS (a few of the short first ones have their answers added to both the Introduction and the Abstract). Replace the text inside the brackets of the command \youranswer with your answer to the question.
%
% There are also 3 "questions" to replace some placeholder FIGURES with your own, and 3 "questions" asking you to fill in the missing entries in the TABLES provided. 
%
% NOTE! that questions are ordered by the order of appearance of their answers in the text, and not by the order you should tackle them. Specifically, you cannot answer Questions 2, 3, and 4 before concluding all of the relevant experiments and analysis. Similarly, you should fill in the TABLES and FIGURES before discussing the results presented there. 
%
% NOTE! If for some reason you do not manage to produce results for some FIGURES and TABLES, then you can get partial marks by discussing your expectations of the results in the relevant TEXT QUESTIONS (for example Question 8 makes use of Table 1 and Figure 2).
%
% Please refer to the coursework specification for more details.


%% - - - - - - - - - - - - TEXT QUESTIONS - - - - - - - - - - - - 

%% Question 1:
\newcommand{\questionOne} {
\youranswer{when a network function is too closely fit to a training
set, and results in an increasing validaiton set error despite a
monotonically decreasing training error(an illustration
 of this concept is shown in figure 1b).}
}

%% Question 2:
\newcommand{\questionTwo} {
\youranswer{can lead to greater perfomrance on validation data,
however also can result in poorer model gernealization due to an increased
likelihood and rate of overfitting when the model is too flexible.}
}

%% Question 3:
\newcommand{\questionThree} {
\youranswer{Question 3 - Summarise what your results show you about the effect of the tested approaches on overfitting and the performance of the trained model}
}

%% Question 4:
\newcommand{\questionFour} {
\youranswer{Question 4 - Give your overall conclusions}
}

%% Question 5:
\newcommand{\questionFive} {
\youranswer{it's validation error begins to increase despite it's training
error decreasing monotonically. This leads to an increasing generealization
gap as the model is trained over a number of epochs. It inidcates that
the model is too flexible and doesn't fit to unseen data very well.}
}

%% Question 6:
\newcommand{\questionSix} {
\youranswer{Overfitting occurs due to a high variance being present within
the trained model. This is due to the model becoming overly complex and
fitting its degrees of freedom in such a way which closely resembles data
points present in the training data. The parameters of the model learn 
the detail and noise in the training data which results in a negative
performance when it comes to the gnerealization of the model and its 
ability to closely predict outputs on unseen data. Overfitting can occur
for a number of reasons. If the learing rate of a model is increased too
much, then it is likely that the parameters of the model will convege and resemble
training data points very closely in a fewer number of epochs respectively. A
clear way to identify if a model is overfit is by visualising the error of
the model on a training and vaildation set across a fixed number of epochs. If 
the validation error begins to increase despite a monotonically decreasing
training error, the model is said to be overfitted. To generalize, if the Model
performs well on a training set, but poorly on unssen data, it is overfitted. 
  }
}

%% Question 7:
\newcommand{\questionSeven} {
\youranswer{it highlights the ratio of total EMNIST samples which were
classified with their target labels correctly over a period of 100 epochs. the
red and blue lines rpresent the accuracy obtained by the baseline model across
the training and test set respectively. It cna be observed that the accuracy
of predictions steeply increased up until epoch 5. Beyond which training accuracy
continued to increase steeply in contrast to the validation accuracy which 
increased at a slower rate and began to level off. At around epoch 16, it
is clear that the model was no longer improving in validation accuracy due to a flat
gradient in the validation accuracy curve. This indicates a maximum optimum
accuracy of 82.5\% was achieved for the model at epoch 16 before it's performance
began to decline. Figure 1b similarly illustrates epoch 16 as the point where
the cross-entropy error measured on the validation set was at it's minimum of
0.5, before it begian to increase. It can be deduced that beyond epoch 16,
training accuracy continued to increase monotonically, whereas validation
accuracy began to decline. A similar pattern can be observed for figure 1b
where the error on the training set conintured to decrease beyond epoch 16,
whereas it increased for the validatoin set. This led to an expanidng
generalization gap. Hence, the baseline model became overfitted
beyond epoch 16. By the end of epoch 100, it can be seeen that the accuracy
across the validation set had fallen from 82.5\% to 80.5\%. Furthermore,
the error across the validation set increased from a minimum of 0.5 to
approximately 0.98. In contrast, by epoch 100, the training set was clearly
fitted to the model due to the high training accuracy of 93.5\% and low
error of 0.17.
}
}

%% Question 8:
\newcommand{\questionEight} {
\youranswer{as the number of hidden units is increased, this does lead to an
imprvement in validation accuracy, as shown in table 1.
The final validation accuracy increased from 78.5\% for 32
units to 80.3\$ for 128 units. We observe that the highest final
validation accuracy of 80.9\% was achieved using 64 hideen units. The reason
why the 128  uint model was unable to outperform the 64 units model is due To
overfititing.
Figure 2a and 2b shows that overfitting of the 128 model is indicated
by the rise in validation error(green line) beytond epoch 18, despite its's corresponding
training error decreasing(yellow line). This led to the decrease in validation accuracy
as shown in figure 2a for acccuraccies recorded beyond epoch 18.
By the end of epoch 100, we can see that the 64 units mode perfromed
almost identically to the 128 ubits model on the validation set.
 The 128 units model had
an advantage when it comes to how quickly the model converged and obtained lower
validation error and higher validation accuracies. It acheived
a maximum validation accuracy of approximately 82.5\% after epoch 18.
It was also the fastest model to see a drastic improvment in validation accuracy.
However due to overfitting, this afvantage didn't last too long. The 32 layer model 
perfomred the worst on the validation set and obtained the lwoest accuracy across the board. This 
is due to the model being underfit and not flexible enough to learn the parameters
needed to fit the training data well. Despite this, it did obtain the lowest
generalixation error of 0.15 so is the least likely model to result in
overfiting. In contrast, the 128 unit model is the most likely to overfit
due to a high geneeralization gap of 0.83.}
}

%% Question 9:
\newcommand{\questionNine} {
\youranswer{Varying the width affects the results in a consistent way as
expected. We expect the model to start off underfitted to the training data 
when less units are used. This is because the model is less
flexible due to having less free parameaters which can fit to to the data.
This is represented well with the lowest validation accutacy 
obtained in table 1 for the 32 units model.
As we increase the number of units. we expect the model to fit the inpuit data
better due to the model having increased flexibiliy. This is demonstrated 
successfully due to the 64 units model having obtained the best final vaildation
accuracy of all the models. Furthermore, we hypthesised that increasing the Model
flexibility too much and having a number of free paramters close to the number
of samples being trained on would result in overfitting due to each free parameters
closely fitting each training point, hence creating noise. This is clerarly 
demonstrated with the 128 units model in figure 2a where it's validation Accuracy
began to decrease despiute the trianing accuracy increasing. It is also clear
the model is overfit due to the increasing error generalization gap which had
formed. 
 }
}

%% Question 10:
\newcommand{\questionTen} {
\youranswer{across all 3 depths tested, the final validation accuracy of
each model increased as the model depth increased. This is primarily due to the
model converging quicker within a fixed amount of epochs as there are
more parameters being fitted to the training data. The consequence of 
increaisng depth is an increased generalizxation error gap. This can be seen from
the spread between the orange and green lines in figure 3a for the model of depth
3. It means although the final validation accuracy increases as depth increases,
the amount the model is overift by increases too hence causes the model to generalize
worse on unseen data. From figure 3b, we can see that the model of depth 1 has
the smallest generalization gap, whereas the model of depth 3 has the largest.
Furthermore, we can observe the effect of increasing depth to be marignal when
it comes to validation accuracy. This can be explained by the model of depth 1
having a similar final
validation accurtacy of 81.2\% compared to the model of depth 2 and 3 which 
had accuraccies of 81.5\% and 82.0\% respectively. Therefore, from figure 3a and
3b we can conclude that increasing model complexity by increasing the depth
is counter-intuitive to imporving the generalization and validation accuracy of
the model. The model genrealizes better with a depth of 1, comparted to depth of
2 and 3. The validation acuttacy gain of using greater depths are marginal and
makes for a worse generalized model.
}
}

%% Question 11:
\newcommand{\questionEleven} {
\youranswer{Vartying depth affects the reuslts in a consistent way as it
increases the amount the model is overfited by. From the previous experiment,
we couild see that using 128 hidden units for a single layer model already
caused the model to become overfitted. By using 128 hidden units per layer, the
model had twice and thrice the amount of parameters to fit to the input data
for depths of 2 and 3 respectively. Therefore, the generalization gap increased 
in proportion to this. The results reflect what is expected as there are a lot
more parameters being fitted to the training data, hence there will be more noise
in the resulting function of the model. This is due to the number of paramters fitted
far exceeding the number of samples being used to train the model on.    )}
}

%% Question 12:
\newcommand{\questionTwelve} {
\youranswer{From the conducted experiments it can be observed that increaisng
the width and depth of the model both reuslted in improved performance. The
final accuracy on unseen data improved when the model complexity was increased
by varying the width and depth. However, it can also be observed that
increases in these parameters can increase the likelihood and rate of
how quickly the model overfits to the training data. This is primarily due To
the increase in model complexity causing the model to overfit to noise in
the training data. As observed, there is a point where increasing model depth
results in marginal gains in perfomrance on unssen data, but reults in poorer
 model generalization. Thus, to prevent overiftting, a compromise
between width and depth needs to be reached to acheive an ideal balance between
model performance and generalization.

}
}

%% Question 13:
\newcommand{\questionThirteen} {
\youranswer{L1/l2 regularisation are methods used to penalise the flexibility
a model has by scaling the models parameters such that it becomes
less flecible.\newline L1 regualarization achieves this by adding a penalty term parameter to
the error function being minimised when training a model. The penalty term added
is dependent on the sum of the magnitude of weight parameters present within
the model. Adding this penalty term reults in the model reducing small weights
to 0 and retaining larger, more important
wieghts that are fundamnetal for the model to generalize well, essentailly acting
as a form of feature selection. L1 regularization can be expklained by the
following equations:

\begin{align}
E^n = E_0 + \beta\sum_w \left\lvert w_i\right\rvert \
\\\frac{\partial E^n}{\partial w_i} = \frac{\partial E_0}{\partial w_i} 
    + \beta  sgn(w_i)\
\end{align}

$E^0$ represents the unregularized error function which is added to the 
sum of the magnitude of all weights present in the model, scaled by a
fixed coeefficent $\beta$. The scaling factor is chosen manually beforehand
and is resposnible for controlling the magnitude of regularization to the
weights. The seocnd equation shows that the updated gradient of the regularized
wrror function with respect to weights now includes a temr dependent on the 
direction of the magnitude of weights. The new updated weight for the model
will take this into account and move in a psoitve or negative direction
proportional to the value of $\beta$.

L2 regularization (commonly known as weight decay) works in a similar way To L1
execpet that the penalty term added is dependent on the sum of the squared
weights as opposed to the sum of the magnitude weights in it's L1 couunterpart.
This reusxlts in the updated wights being dependent on the size of the weights.
Therefore, the gradients shrink at a rate proportional to this. Thus, this means
weightd are pusehjd to be as small as possible as long as the regularized cost 
function continues to decrease. L2 regularization can be desicrbed by the
following equations:

\begin{align}
E^n = E_0 + \frac{\beta}{2}\sum_w {w_i}^2
\\\frac{\partial E^n}{\partial w_i} = \frac{\partial E_0}{\partial w_i} 
     + \beta  w_i\
\end{align}

From the seocnd equation, we can see the updated wieght will now be dpendent on an 
anditional term which has a value propoortional to the sixe of the current weight
The regularization techinques are incorporated in trianing by ussing a chosen
hyperparamter $/beta$ which will be incoroporated into the fomrulas above. The
corresponding regularization fomula will then be computed on the ranodmly initialised weights for each chosen
layer in the model. In order to update the gradient, backpropogation will be used
in the same way as it is used to caluclate te gradient of the cost function with
respect to the weights. The only differnce is that the corrsponing penalty
gradient will be added on during the packrpopogation phase of training. In l2
regularization, a bias penalty is often not used due to large bieas values 
being potentially useful for the model as it allows for additional flexibility
\cite{NNandDL2015}. Furthermore, having large biases does not make the neurons
inputs sensitice in the same way weights do.
}
}


%% Question 14:
\newcommand{\questionFourteen} {
\youranswer{The weigth pentaltes help to address overfitting as it reduces the 
flexibility and sensitivity of the model by making the weights smaller. This in turn means The
model is less likelly to fit to noise within the training data. Large values
will be multiplied by small weight vectors so the output reuslt is less likely
to deviate far from the target label. If no regularization was used, ther weights
are unbounded and are free to diverge and become very large which leads to The
free parameters being prone to fitting to noise. Rgularization almost acts as a form of
smoothing for the trained parameters and prevents them from becoming too large
which would amke the model sensitive to small changes in the inpuit. Regualrization
helps to keep the model simpler and gices it a chance to learn patterns commonly
present within the trianing data, hence leads to better generalization.\newline
    
The main differnce between L1 and L2 are higihlighted by thier gradient terms.
In L1 regualrization it can be seen that weights will always be updated at a
constant rate with the only vairation being the direction in which the gradient
is updated. With L2 regularization, the weights will be updated at
changing rates due to the updated gradient being reliant on thr size of the current
weights. As mentioned in \cite{NNandDL2015}, the main differneces bettwen both
types of regularizations are the weights that are targetted. For a model with 
large weights, L11 will shrink the weights slower than L2 due to L2's shrink
factor being larger than L1's. For a model containing smaller weights, L2 will shirnk
a lot slower than L1. With L1, small weights are shrunk to 0, whilst a small
number of high importance weight neurons are retained. }
}

%% Question 15:
\newcommand{\questionFifteen} {
\youranswer{Question 15 - Explain the experimental details (e.g. hyperparameters), discuss the results in terms of their generalization performance and overfitting}
}

%% Question 16:
\newcommand{\questionSixteen} {
\youranswer{Question 16 - Explain the motivation behind Maxout Networks as presented in \cite{goodfellow2013maxout}}
}

%% Question 17:
\newcommand{\questionSeventeen} {
\youranswer{Question 17 - State whether Dropout is compatible (can be used together) with Maxout and explain why}
}

%% Question 18:
\newcommand{\questionEighteen} {
\youranswer{Question 18 - Give an overview of the experiment setup in \cite{goodfellow2013maxout} and analyse it from the point of view of how convincing their conclusions are}
}

%% Question 19:
\newcommand{\questionNineteen} {
\youranswer{Question 19 - Briefly draw your conclusions based on the results from the previous sections (what are the take-away messages?) and conclude your report with a recommendation for future directions}
}


%% - - - - - - - - - - - - FIGURES - - - - - - - - - - - - 

%% Question Figure 2:
\newcommand{\questionFigureTwo} {
\youranswer{Question Figure 2 - Replace the images in Figure 2 with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden units.
%
\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/task1_hu_accuracy.pdf}
        \caption{accuracy by epoch}
        \label{fig:width_acccurves}
    \end{subfigure} 
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/task1_hu_error.pdf}
        \caption{error by epoch}
        \label{fig:width_errorcurves}
    \end{subfigure} 
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network widths.}
    \label{fig:width}
\end{figure} 
}
}

%% Question Figure 3:
\newcommand{\questionFigureThree} {
\youranswer{Question Figure 3 - Replace these images with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden layers.
%
\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/task1_vlayers_accuracy.pdf}
        \caption{accuracy by epoch}
        \label{fig:depth_acccurves}
    \end{subfigure} 
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/task1_vlayers_error.pdf}
        \caption{error by epoch}
        \label{fig:depth_errorcurves}
    \end{subfigure} 
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network depths.}
    \label{fig:depth}
\end{figure} 
}
}

%% Question Figure 4:
\newcommand{\questionFigureFour} {
\youranswer{Question Figure 4 - Replace these images with figures depicting the Validation Accuracy and Generalisation Gap for each of your experiments varying the Dropout inclusion rate, L1/L2 weight penalty, and for the 8 combined experiments (you will have to find a way to best display this information in one subfigure).
%
\begin{figure*}[t]
    \centering
    \begin{subfigure}{.3\linewidth}
        \includegraphics[width=\linewidth]{figures/empty_dropout_plot.png}
        \caption{Metrics by inclusion rate}
        \label{fig:dropoutrates}
    \end{subfigure} 
    \begin{subfigure}{.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/empty_wd_plot.png}
        \caption{Metrics by weight penalty}
        \label{fig:weightrates}
    \end{subfigure} 
    \begin{subfigure}{.3\linewidth}
        \centering
        \includegraphics[width=.85\linewidth]{example-image-duck}
        \caption{Extra experiments}
        \label{fig:extra}
    \end{subfigure} 
    \caption{Hyperparameter search for every method and combinations}
    \label{fig:hp_search}
\end{figure*}
}
}

%% - - - - - - - - - - - - TABLES - - - - - - - - - - - - 

%% Question Table 1:
\newcommand{\questionTableOne} {
\youranswer{
Question Table 1 - Fill in Table 1 with the results from your experiments varying the number of hidden units.
%
\begin{table}[t]
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \# hidden units & val. acc. & generalization gap \\
    \midrule
         32            &        78.5    &            0.15        \\
         64            &        80.9   &             0.32       \\
         128           &        80.3  &              0.83      \\ 
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and generalization gap (in terms of cross-entropy error) for varying network widths on the EMNIST dataset.}
    \label{tab:width_exp}
\end{table}
}
}

%% Question Table 2:
\newcommand{\questionTableTwo} {
\youranswer{
Question Table 2 - Fill in Table 2 with the results from your experiments varying the number of hidden layers.
%
\begin{table}[t]
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \# hidden layers & val. acc. & generalization gap \\
    \midrule
         1               &      81.2      &       0.78             \\
         2               &      81.5      &        1.46            \\
         3               &      82.0     &         1.58           \\ 
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and generalization gap (in terms of cross-entropy error) for varying network depths on the EMNIST dataset.}
    \label{tab:depth_exps}
\end{table}
}
}

%% Question Table 3:
\newcommand{\questionTableThree} {
\youranswer{
Question Table 3 - Fill in Table 3 with the results from your experiments varying the hyperparameter values for each of L1 regularisation, L2 regularisation, and Dropout (use the values shown on the table) as well as the results for your experiments combining L1/L2 and Dropout (you will have to pick what combinations of hyperparameter values to test for the combined experiments; each of the combined experiments will need to use Dropout and either L1 or L2 regularisation; run an experiment for each of 8 different combinations). Use \textit{italics} to print the best result per criterion for each set of experiments, and \textbf{bold} for the overall best result per criterion.
%
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|cc}
    \toprule
        Model    &  Hyperparameter value(s) & Validation accuracy & Generalization gap \\
    \midrule
    \midrule
        Baseline &  -                    &               0.836 &                 0.290 \\
    \midrule
        \multirow{3}*{Dropout}
                 & 0.7                   &                0.817     &      0.030             \\
                 & 0.9                   &                0.856    &           0.090        \\
                 & 0.95                  &               0.861      &      0.140             \\
    \midrule
        \multirow{3}*{L1 penalty}
                 & 1e-4                   &        0.847             &     0.08              \\
                 & 1e-3                   &        0.751             &      0.01             \\
                 & 1e-1                   &        0.0246             &          0.00         \\
    \midrule
        \multirow{3}*{L2 penalty}  
                 & 1e-4                   &          0.845           &    0.24               \\
                 & 1e-3                   &          0.849           &   0.09                \\
                 & 1e-1                   &         0.0198            &   0.00                \\
    \midrule
        \multirow{6}*{Combined}  
                 & 0.95, L2: 1e-4  &                     &                   \\
                 & 0.90, L1: 1e-4                   &                     &                   \\
                 & 0.95, L1: 1e-4                   &                     &                   \\
                 & 0.95, L2: 1e-3                   &                     &                   \\
                 & 0.90, L2: 1e-3                   &                     &                   \\
                 & 0.90, L2: 1e-4                  &                     &                   \\
    \bottomrule
    \end{tabular}
    \caption{Results of all hyperparameter search experiments. \emph{italics} indicate the best results per series and \textbf{bold} indicate the best overall}
    \label{tab:hp_search}
\end{table*}
}
}

%% END of YOUR ANSWERS