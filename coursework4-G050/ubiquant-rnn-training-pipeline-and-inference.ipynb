{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThis is my starter notebook for how to start with a recurrent neural network in this competition. It has much room for improvement, but hopefully this gives you a baseline to work with. \n\nThis notebook currently score .142 on the latest version. I also have a lgbm training pipeline in this notebook that scores a .133 but it is not used in this inference.\n\nThe training pipelines are attached, the models were trained offline, and the weights are attached for inference.\n\n## Model\n- 300 feature input\n- Batch Normalization\n- Dense feature extractor with dropout\n- Reshape and Batch Normalization to setup for RNN\n- LSTM layers\n- Dense head\n\nSee diagram below\n\n## Callbacks\n- ReduceLROnPlateau\n- ModelCheckpoint\n- EarlyStopping\n\n## Future Ideas\n- add new input features\n- add investment_id as an input with an embedding layer\n- use attention\n- try different callbacks, learning rates, loss functions, etc\n- try 1-D CNN","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, display\ndisplay(Image(filename=\"../input/ubiquant-models/saved_models/images/rnn_v2.png\", width = 210, height = 65))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:00:55.884217Z","iopub.execute_input":"2022-04-01T17:00:55.884483Z","iopub.status.idle":"2022-04-01T17:00:55.907234Z","shell.execute_reply.started":"2022-04-01T17:00:55.884443Z","shell.execute_reply":"2022-04-01T17:00:55.906614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport pickle\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GroupKFold\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-01T17:40:07.140924Z","iopub.execute_input":"2022-04-01T17:40:07.141207Z","iopub.status.idle":"2022-04-01T17:40:14.814201Z","shell.execute_reply.started":"2022-04-01T17:40:07.141175Z","shell.execute_reply":"2022-04-01T17:40:14.813446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation","metadata":{}},{"cell_type":"code","source":"def setup_cv(df, X, y, groups, splits=5):\n    kf = GroupKFold(n_splits=splits)\n    for f, (t_, v_) in enumerate(kf.split(X=X, y=y, groups=groups)):\n            df.loc[v_, 'fold'] = f\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:40:18.027285Z","iopub.execute_input":"2022-04-01T17:40:18.027687Z","iopub.status.idle":"2022-04-01T17:40:18.041258Z","shell.execute_reply.started":"2022-04-01T17:40:18.027644Z","shell.execute_reply":"2022-04-01T17:40:18.040262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\n\n## RNN","metadata":{}},{"cell_type":"code","source":"def get_rnn_v2():\n    f300_in = L.Input(shape=(300,), name='300 feature input')\n    x = L.BatchNormalization(name='batch_norm1')(f300_in)\n    x = L.Dense(256, activation='swish', name='dense1')(x)\n    x = L.Dropout(0.1, name='dropout1')(x)\n    x = L.Reshape((1, -1), name='reshape1')(x)\n    x = L.BatchNormalization(name='batch_norm2')(x)\n    x = L.LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='relu', name='lstm1')(x)\n    x = L.LSTM(16, dropout=0.1, return_sequences=False, activation='relu', name='lstm2')(x)\n    output_layer = L.Dense(1, name='output')(x)\n\n    model = M.Model([f300_in], \n                    [output_layer])\n\n    model.compile(optimizer=tf.optimizers.Adam(lr=0.001),\n                  loss='mse', metrics=['mse'])\n\n    return model\n\nclass UbiquantRNNV2:\n    def __init__(self, df: pd.DataFrame, feature_cols: list=None, target: str='target'):\n\n        self.model = get_rnn_v2()\n\n        self.df = df\n\n        if feature_cols is not None:\n            self.feature_cols = feature_cols\n        else:\n            self.feature_cols = [f\"f_{i}\" for i in range(300)]\n\n        self.target_col = target\n\n    def train_one_fold(self, f: int, max_epochs=10):\n        X_train = self.df[self.df.fold!=f][self.feature_cols]\n        X_valid = self.df[self.df.fold==f][self.feature_cols]\n\n        y_train = self.df[self.df.fold!=f][self.target_col]\n        y_valid = self.df[self.df.fold==f][self.target_col]\n\n        self.model.fit(X_train, y_train,\n                       validation_data=(X_valid, y_valid),\n                       batch_size=128, epochs=max_epochs,\n                       callbacks=[\n                         ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min'),\n                         ModelCheckpoint(f'RNN_v2_checkpoint_{f}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min'),\n                         EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)\n            ])\n\n        oof = self.model.predict(X_valid)\n        oof_score = np.sqrt(mean_squared_error(y_valid, oof))\n        print(f'oof rmse: {oof_score}')\n\n    def predict(self, X: np.ndarray):\n        preds = self.model.predict(X)\n        return preds\n\n    def save(self, path: str):\n        pickle.dump(self.model, open(path, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:40:22.283704Z","iopub.execute_input":"2022-04-01T17:40:22.284022Z","iopub.status.idle":"2022-04-01T17:40:22.302634Z","shell.execute_reply.started":"2022-04-01T17:40:22.283988Z","shell.execute_reply":"2022-04-01T17:40:22.301659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM","metadata":{}},{"cell_type":"code","source":"class UbiquantLGBM:\n    \"\"\"\n    This class is the Training Pipeline for an LGBM Regressor\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, feature_cols: list=None, target: str='target'):\n        \"\"\" Creates the pipeline \"\"\"\n        params = {\n            'random_state': 42, \n            'verbosity': -1,\n            'metrics': 'rmse',\n        }  \n        self.model = LGBMRegressor(**params)\n\n        self.df = df\n\n        if feature_cols is not None:\n            self.feature_cols = feature_cols\n        else:\n            self.feature_cols = [f\"f_{i}\" for i in range(300)]\n\n        self.target_col = target\n\n    def train_one_fold(self, f: int):\n        \"\"\" Trains one fold of the lgbm \"\"\"\n        X_train = self.df[self.df.fold!=f][self.feature_cols]\n        X_valid = self.df[self.df.fold==f][self.feature_cols]\n\n        y_train = self.df[self.df.fold!=f][self.target_col]\n        y_valid = self.df[self.df.fold==f][self.target_col]\n\n        self.model.fit(X_train, y_train, \n                       eval_set=[(X_valid, y_valid)],\n                       eval_metric='rmse',\n                       verbose=False,\n                       early_stopping_rounds=30)\n\n        oof = self.model.predict(X_valid)\n        oof_score = np.sqrt(mean_squared_error(y_valid, oof))\n        print(f'oof rmse: {oof_score}')\n\n    def predict(self, X: np.ndarray):\n        \"\"\" Makes a prediction with the model \"\"\"\n        preds = self.model.predict(X)\n        return preds\n\n    def save(self, path: str):\n        \"\"\"Saves the model \"\"\"\n        pickle.dump(self.model, open(path, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:11:06.113927Z","iopub.execute_input":"2022-04-01T12:11:06.114422Z","iopub.status.idle":"2022-04-01T12:11:06.129048Z","shell.execute_reply.started":"2022-04-01T12:11:06.114375Z","shell.execute_reply":"2022-04-01T12:11:06.128196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def load_model(file_path):\n    \"\"\" Loads a model pipeline object \"\"\"\n    file = open(file_path,'rb')\n    model = pickle.load(file)\n    file.close()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:40:26.966159Z","iopub.execute_input":"2022-04-01T17:40:26.966899Z","iopub.status.idle":"2022-04-01T17:40:26.974383Z","shell.execute_reply.started":"2022-04-01T17:40:26.966857Z","shell.execute_reply":"2022-04-01T17:40:26.973409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Define our model with the trained weights \"\"\"\nrnn0 = get_rnn_v2()\nrnn0.load_weights(\"../input/ubiquant-models/saved_models/rnn_checkpoints/RNN_v2_checkpoint_0.hdf5\")\nrnn1 = get_rnn_v2()\nrnn1.load_weights(\"../input/ubiquant-models/saved_models/rnn_checkpoints/RNN_v2_checkpoint_1.hdf5\")\nrnn2 = get_rnn_v2()\nrnn2.load_weights(\"../input/ubiquant-models/saved_models/rnn_checkpoints/RNN_v2_checkpoint_2.hdf5\")\nrnn3 = get_rnn_v2()\nrnn3.load_weights(\"../input/ubiquant-models/saved_models/rnn_checkpoints/RNN_v2_checkpoint_3.hdf5\")\nrnn4 = get_rnn_v2()\nrnn4.load_weights(\"../input/ubiquant-models/saved_models/rnn_checkpoints/RNN_v2_checkpoint_4.hdf5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:40:29.31807Z","iopub.execute_input":"2022-04-01T17:40:29.31868Z","iopub.status.idle":"2022-04-01T17:40:33.308987Z","shell.execute_reply.started":"2022-04-01T17:40:29.318636Z","shell.execute_reply":"2022-04-01T17:40:33.308166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\nf_col = df.drop(['row_id','time_id','investment_id','target'],axis=1).columns\nf_col","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:40:35.912605Z","iopub.execute_input":"2022-04-01T17:40:35.913186Z","iopub.status.idle":"2022-04-01T17:41:17.196904Z","shell.execute_reply.started":"2022-04-01T17:40:35.913143Z","shell.execute_reply":"2022-04-01T17:41:17.196175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(df):\n    inv_df = df['investment_id']\n    f_df = df[f_col]\n    scaled_investment_id = scaler.transform(pd.DataFrame(inv_df))\n    df['investment_id'] = scaled_investment_id\n    #data_x = pd.concat([df['investment_id'], f_df], axis=1)\n    return f_df","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:41:25.518178Z","iopub.execute_input":"2022-04-01T17:41:25.518887Z","iopub.status.idle":"2022-04-01T17:41:25.524126Z","shell.execute_reply.started":"2022-04-01T17:41:25.518843Z","shell.execute_reply":"2022-04-01T17:41:25.523147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(pd.DataFrame(df['investment_id']))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:41:52.472029Z","iopub.execute_input":"2022-04-01T17:41:52.47256Z","iopub.status.idle":"2022-04-01T17:41:52.541971Z","shell.execute_reply.started":"2022-04-01T17:41:52.472517Z","shell.execute_reply":"2022-04-01T17:41:52.541211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.astype('float16')\ndf_x = make_dataset(df)\ndf_x","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:41:54.764939Z","iopub.execute_input":"2022-04-01T17:41:54.765435Z","iopub.status.idle":"2022-04-01T17:42:04.845617Z","shell.execute_reply.started":"2022-04-01T17:41:54.765394Z","shell.execute_reply":"2022-04-01T17:42:04.844846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_y = pd.DataFrame(df['target'])\ndf_y","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:42:21.343603Z","iopub.execute_input":"2022-04-01T17:42:21.343896Z","iopub.status.idle":"2022-04-01T17:42:21.357582Z","shell.execute_reply.started":"2022-04-01T17:42:21.343864Z","shell.execute_reply":"2022-04-01T17:42:21.356832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:42:23.855963Z","iopub.execute_input":"2022-04-01T17:42:23.856468Z","iopub.status.idle":"2022-04-01T17:42:23.864959Z","shell.execute_reply.started":"2022-04-01T17:42:23.856426Z","shell.execute_reply":"2022-04-01T17:42:23.864189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold_generator = KFold(n_splits =5, shuffle=True, random_state = 2022)\nkfold_generator\n\n# Write your model name down in 'pythonash_model.h5'.\ncallbacks = tf.keras.callbacks.ModelCheckpoint('LSTM_model.h5', save_best_only = True)\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = get_rnn_v2()\n    # Model fitting\n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train,\n                       validation_data=(tf_val),\n                       batch_size=128, epochs=10,\n                       callbacks=[\n                         ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min'),\n                         EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)\n            ])\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model","metadata":{"execution":{"iopub.status.busy":"2022-04-01T17:42:26.779568Z","iopub.execute_input":"2022-04-01T17:42:26.78019Z","iopub.status.idle":"2022-04-01T18:12:00.815385Z","shell.execute_reply.started":"2022-04-01T17:42:26.780147Z","shell.execute_reply":"2022-04-01T18:12:00.814199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Make predictions for competition \"\"\"\nimport ubiquant\nfeats = [f\"f_{i}\" for i in range(300)]\n\n#env = ubiquant.make_env()   \n#iter_test = env.iter_test()    \n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_300 = test_df[feats]\n    test_invest_id = test_df[['investment_id']]\n    \n    pred0 = rnn0.predict(test_300)\n    pred1 = rnn1.predict(test_300)\n    pred2 = rnn2.predict(test_300)\n    pred3 = rnn3.predict(test_300)\n    pred4 = rnn4.predict(test_300)\n    pred = np.mean([pred0, pred1, pred2, pred3, pred4], axis=0)\n    \n    sample_prediction_df['target'] = pred\n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:13:16.532958Z","iopub.execute_input":"2022-04-01T12:13:16.533296Z","iopub.status.idle":"2022-04-01T12:13:16.541248Z","shell.execute_reply.started":"2022-04-01T12:13:16.533256Z","shell.execute_reply":"2022-04-01T12:13:16.54049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T12:13:20.851204Z","iopub.execute_input":"2022-04-01T12:13:20.851866Z","iopub.status.idle":"2022-04-01T12:13:20.865407Z","shell.execute_reply.started":"2022-04-01T12:13:20.851817Z","shell.execute_reply":"2022-04-01T12:13:20.864381Z"},"trusted":true},"execution_count":null,"outputs":[]}]}